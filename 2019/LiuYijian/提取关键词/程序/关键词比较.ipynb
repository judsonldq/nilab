{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/x_/43mxrf_j66q5ynk5hsxw14lw0000gn/T/jieba.cache\n",
      "Loading model cost 0.764 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "#-*- encoding:utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from gensim.models import word2vec\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import jieba\n",
    "jieba.load_userdict('/Users/lyj/Desktop/提案程序/sogou.txt')\n",
    "jieba.load_userdict('/Users/lyj/Desktop/提案程序/标题补充词典.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取stop停用词  \n",
    "def readstopwords(stopwords_path): \n",
    "    stop_single_words=[]  \n",
    "    with open(stopwords_path,'r+',encoding='utf-8') as lines:\n",
    "        for line in lines:  \n",
    "            content=line.strip()  \n",
    "            stop_single_words.append(content) \n",
    "    return stop_single_words\n",
    "\n",
    "#把文档中的所有文件名字存入数组\n",
    "def getFilelist(path):\n",
    "    txtNames = []                     #所有txt文档名字\n",
    "    pathDir = os.listdir(path) #把文件夹里的所有txt文档的名字都获取了，放入数组\n",
    "    for allDir in pathDir:\n",
    "        if(allDir.find('txt') >= 0 and allDir.find('result') < 0):\n",
    "            txtNames.append(allDir)\n",
    "    return txtNames \n",
    "\n",
    "#具体的分词程序\n",
    "def fenci(filePath,filename,stop_single_words) :\n",
    "    f = open(filePath+'/'+filename,'r+',encoding='utf-8')\n",
    "    file_list = f.read()\n",
    "    f.close()\n",
    "    #正则化必须去除字母数字百分数，不然计算的TF-IDF在用来求提案质心时会出错\n",
    "    #错误类型：ValueError: 'GDP' is not in list\n",
    "    file_list=re.sub('[A-Za-z0-9,\\d+(\\.\\d+)?\\%]','',file_list) #去除字母,数字,百分数\n",
    "    #file_list=re.sub('[0-9,\\d+(\\.\\d+)?\\%]','',file_list)\n",
    "    \n",
    "    lis=[] #保存分词结果\n",
    "    seg_list = jieba.cut(file_list,cut_all=False)\n",
    "    for seg in seg_list : \n",
    "        seg = ''.join(seg.split())\n",
    "        if seg not in stop_single_words and len(seg)!=1: #为什么计算质心的时候不能有单字词\n",
    "            lis.append(seg)  \n",
    "    return lis\n",
    "\n",
    "#保存每篇提案的分词结果\n",
    "def save_word_seg_result(filename,word_seg_result,save_word_seg_path):\n",
    "    f = open(save_word_seg_path+\"/\"+filename,\"w+\",encoding='utf-8')\n",
    "    f.write(' '.join(word_seg_result))\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "#对整个文档分词\n",
    "def Participle(filePath,key_path,save_word_seg_path,stop_single_words):   \n",
    "    word_count={}    #存放所有提案的 词-数量 键值对\n",
    "    reserved_words=[] #存放用于计算质心的词，也就是用于训练词向量的词\n",
    "    doc_name_words={} #保存所有提案的 名字-分词结果 键值对\n",
    "    sentences=[] #嵌套列表，保存每篇提案的分词结果\n",
    "    allfile = getFilelist(filePath) #获取所有提案的文件名\n",
    "    allfile1 = getFilelist(key_path)\n",
    "    allfile.extend(allfile1)\n",
    "    \n",
    "    for file in allfile:\n",
    "        try:\n",
    "            word_seg_result=fenci(filePath,file,stop_single_words)  #对每个提案分词\n",
    "        except:\n",
    "            word_seg_result=fenci(key_path,file,stop_single_words)\n",
    "        sentences.append(word_seg_result)\n",
    "        for word in word_seg_result:  #统计所有提案中每个词的数量\n",
    "            if word not in word_count.keys():\n",
    "                word_count[word]=1\n",
    "            else:\n",
    "                word_count[word]+=1\n",
    "        doc_name_words[file]=word_seg_result #将提案名和词按字典保存\n",
    "    doc_name_words_new={}\n",
    "    for key in doc_name_words.keys():\n",
    "        word_list=[]\n",
    "        for word_ in doc_name_words[key]:\n",
    "            if word_count[word_]>1:\n",
    "                word_list.append(word_)\n",
    "        #保存分词结果，平时注释掉\n",
    "        save_word_seg_result(key,word_list,save_word_seg_path) \n",
    "        doc_name_words_new[key]=word_list\n",
    "    return doc_name_words_new,sentences,allfile\n",
    "\n",
    "#训练词向量\n",
    "def train_vec(sentences,dimen):\n",
    "    model = word2vec.Word2Vec(sentences,min_count=2,size=dimen) #训练词向量\n",
    "    return model\n",
    "\n",
    "#读取800份已分词好的文档，进行TF-IDF计算。\n",
    "#这里是去除停用词的所有词，且删除了出现次数小于某一阈值的词\n",
    "def Tfidf(path,filelist):\n",
    "    corpus = []  #存取800份文档的分词结果\n",
    "    for ff in filelist :\n",
    "        fname = path +\"/\"+ff\n",
    "        f = open(fname,\"r+\",encoding=\"utf-8\")\n",
    "        content = f.read()\n",
    "        corpus.append(content)\n",
    "        f.close()\n",
    "\n",
    "    vectorizer = CountVectorizer()    \n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "    tfidf_word = vectorizer.get_feature_names() #所有文本的分词词语\n",
    "    tfidf_weight = tfidf.toarray()              #对应的tfidf矩阵\n",
    "    return tfidf_word,tfidf_weight.tolist()\n",
    "\n",
    "#计算每篇文档的质心\n",
    "#doc_name_words中的词删除了停用词和小于某一阈值（出现次数小于1）的词\n",
    "#reserved_words中的词删除了停用词和出现次数小于某一阈值的词\n",
    "def centroid_TF(doc_name_words,model,allfile,tfidf_word,tfidf_weight,dimen):\n",
    "    doc_vec_list=[]\n",
    "    doc_vec_dic={}\n",
    "    for name in doc_name_words.keys():\n",
    "        sum_value=np.mat(zeros((1,dimen)))\n",
    "        count=0\n",
    "        for word in doc_name_words[name]:\n",
    "            weight=tfidf_weight[allfile.index(name)][tfidf_word.index(word)]\n",
    "            count+=1\n",
    "            sum_value+=np.mat(model[word])*weight #词向量*权重\n",
    "            #print(tfidf_weight[allfile.index(name)][tfidf_word.index(word)])\n",
    "        doc_vec_list.append(((sum_value/count).tolist())[0]) #这两处必须转化为列表，如果是数组，后面聚类程序保存文件的循环出错\n",
    "        doc_vec_dic[name]=((sum_value/count).tolist())[0]\n",
    "    return doc_vec_list,doc_vec_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:112: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814\n"
     ]
    }
   ],
   "source": [
    "stopwords=\"/Users/lyj/Desktop/提案程序/百度停用词列表（去除英文）+tycb_symbol.txt\"  \n",
    "filePath = \"/Users/lyj/Desktop/提案程序/提案数据\"  #txt文本集所在的文件夹路径\n",
    "key_path=\"/Users/lyj/Desktop/关键词文档\"\n",
    "save_word_seg_path=\"/Users/lyj/Desktop/提案程序/关键词文档\" #保存分词结果的路径\n",
    "\n",
    "stop_single_words=readstopwords(stopwords)\n",
    "doc_name_words,sentences,allfile=Participle(filePath,key_path,save_word_seg_path,stop_single_words) #对整个文档分词\n",
    "tfidf_word,tfidf_weight=Tfidf(save_word_seg_path,allfile)\n",
    "dimen=300 #词向量维度\n",
    "model=train_vec(sentences,dimen)\n",
    "#求每篇文档的质心\n",
    "#这是一条说明语句doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,threshold,weight,title_seg_lis)\n",
    "doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,model,allfile,tfidf_word,tfidf_weight,dimen)\n",
    "print(len(doc_vec_list))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "短词，主题1的微博文档与主题1对应提案的欧式距离: 0.197    主题1的提案数: 68\n",
      "短词，主题2的微博文档与主题2对应提案的欧式距离: 0.187    主题2的提案数: 78\n",
      "短词，主题3的微博文档与主题3对应提案的欧式距离: 0.533    主题3的提案数: 3\n",
      "短词，主题4的微博文档与主题4对应提案的欧式距离: 0.138    主题4的提案数: 432\n",
      "短词，主题5的微博文档与主题5对应提案的欧式距离: 0.181    主题5的提案数: 85\n",
      "短词，主题6的微博文档与主题6对应提案的欧式距离: 0.18    主题6的提案数: 57\n",
      "短词，主题7的微博文档与主题7对应提案的欧式距离: 0.172    主题7的提案数: 60\n",
      "短词，主题8的微博文档与主题8对应提案的欧式距离: 0.13    主题8的提案数: 15\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "key_path_1=\"/Users/lyj/Desktop/关键词文档1\"\n",
    "cluster_out_path=\"/Users/lyj/Desktop/提案程序/word2veccluster（终极版）\"\n",
    "key_name = getFilelist(key_path_1)\n",
    "cluster_out_name=getFilelist(cluster_out_path)\n",
    "dic1={}\n",
    "for name in key_name:\n",
    "    for file_name in cluster_out_name:\n",
    "        if name==file_name:\n",
    "            l_list=[]\n",
    "            doc_name_list = getFilelist(cluster_out_path+\"/\"+file_name)\n",
    "            for name2 in doc_name_list:\n",
    "                l=np.sqrt(np.sum(np.square(np.mat(doc_vec_dic[name]) - np.mat(doc_vec_dic[name2]))))\n",
    "                #dist = pdist(np.vstack([np.mat(doc_vec_dic[name]),np.mat(doc_vec_dic[name2])]),'cosine')\n",
    "                l_list.append(l)\n",
    "    if name==\"1.txt\":\n",
    "        dic1[1]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"2.txt\":\n",
    "        dic1[2]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"3.txt\":\n",
    "        dic1[3]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"4.txt\":\n",
    "        dic1[4]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"5.txt\":\n",
    "        dic1[5]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"6.txt\":\n",
    "        dic1[6]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"7.txt\":\n",
    "        dic1[7]=[round(mean(l_list),3),len(l_list)]\n",
    "    if name==\"8.txt\":\n",
    "        dic1[8]=[round(mean(l_list),3),len(l_list)]\n",
    "        \n",
    "for iii in sorted(dic1.items(),key = lambda x:x[0]):\n",
    "    print(\"短词，主题\"+str(iii[0])+\"的微博文档与主题\"+str(iii[0])+\"对应提案的欧式距离:\",iii[1][0],\"  \",\"主题\"+str(iii[0])+\"的提案数:\",iii[1][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长词，主题1的微博文档与主题1对应提案的欧式距离: 0.173    主题1的提案数: 68\n",
      "长词，主题2的微博文档与主题2对应提案的欧式距离: 0.162    主题2的提案数: 78\n",
      "长词，主题3的微博文档与主题3对应提案的欧式距离: 0.376    主题3的提案数: 3\n",
      "长词，主题4的微博文档与主题4对应提案的欧式距离: 0.123    主题4的提案数: 432\n",
      "长词，主题5的微博文档与主题5对应提案的欧式距离: 0.164    主题5的提案数: 85\n",
      "长词，主题6的微博文档与主题6对应提案的欧式距离: 0.175    主题6的提案数: 57\n",
      "长词，主题7的微博文档与主题7对应提案的欧式距离: 0.14    主题7的提案数: 60\n",
      "长词，主题8的微博文档与主题8对应提案的欧式距离: 0.125    主题8的提案数: 15\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "key_path_2=\"/Users/lyj/Desktop/关键词文档2\"\n",
    "cluster_out_path2=\"/Users/lyj/Desktop/提案程序/word2veccluster（终极版）的副本\"\n",
    "key_name2 = getFilelist(key_path_2)\n",
    "cluster_out_name2=getFilelist(cluster_out_path2)\n",
    "dic2={}\n",
    "for name1 in key_name2:\n",
    "    for file_name2 in cluster_out_name2:\n",
    "        if name1==file_name2:\n",
    "            l_list2=[]\n",
    "            doc_name_list2 = getFilelist(cluster_out_path2+\"/\"+file_name2)\n",
    "            for name22 in doc_name_list2:\n",
    "                l2=np.sqrt(np.sum(np.square(np.mat(doc_vec_dic[name1]) - np.mat(doc_vec_dic[name22]))))\n",
    "                #dist2 = pdist(np.vstack([np.mat(doc_vec_dic[name1]),np.mat(doc_vec_dic[name22])]),'cosine')\n",
    "                l_list2.append(l2)\n",
    "    if name1==\"11.txt\":\n",
    "        dic2[1]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"22.txt\":\n",
    "        dic2[2]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"33.txt\":\n",
    "        dic2[3]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"44.txt\":\n",
    "        dic2[4]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"55.txt\":\n",
    "        dic2[5]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"66.txt\":\n",
    "        dic2[6]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"77.txt\":\n",
    "        dic2[7]=[round(mean(l_list2),3),len(l_list2)]\n",
    "    if name1==\"88.txt\":\n",
    "        dic2[8]=[round(mean(l_list2),3),len(l_list2)]\n",
    "        \n",
    "for iii2 in sorted(dic2.items(),key = lambda x:x[0]):\n",
    "    print(\"长词，主题\"+str(iii2[0])+\"的微博文档与主题\"+str(iii2[0])+\"对应提案的欧式距离:\",iii2[1][0],\"  \",\"主题\"+str(iii2[0])+\"的提案数:\",iii2[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11868f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 \n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    " \n",
    "name_list = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','topic_7','topic_8']\n",
    "num_list = [0.197,0.187,0.533,0.138,0.181,0.180,0.172,0.13]\n",
    "num_list1 = [0.173,0.162,0.376,0.123,0.164,0.175,0.140,0.125]\n",
    "x =list(range(len(num_list)))\n",
    "total_width, n = 0.8, 2\n",
    "width = total_width / n\n",
    " \n",
    "plt.bar(x, num_list, width=width, label='Short word',fc = 'b')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, num_list1, width=width, label='Long word',tick_label = name_list,fc = 'r')\n",
    "#plt.yticks([0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# ax.set_title(\"Similarity between Weibo data of different topics and corresponding proposals\")\n",
    "# ax.set_xlabel(\"topic\")\n",
    "# ax.set_ylabel(\"European distance\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
