{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整个文档聚类；聚类数，阈值，权重选择；所有文档出现次数大于1的词训练词向量；加权平均计算质心；同一个簇中的文档中的每个词权重相加，作为该词的权重，提取四字词作为关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#-*- encoding:utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import word2vec\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import jieba\n",
    "#jieba.load_userdict('/Users/lyj/Documents/项目/补充词典/word2.txt')\n",
    "jieba.load_userdict('/Users/lyj/Desktop/提案程序/sogou.txt')\n",
    "jieba.load_userdict('/Users/lyj/Desktop/提案程序/标题补充词典.txt')\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.interpolate import spline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#读取stop停用词  \n",
    "def readstopwords(stopwords_path): \n",
    "    stop_single_words=[]  \n",
    "    with open(stopwords_path,'r+',encoding='utf-8') as lines:\n",
    "        for line in lines:  \n",
    "            content=line.strip()  \n",
    "            stop_single_words.append(content) \n",
    "    return stop_single_words\n",
    "\n",
    "#把文档中的所有文件名字存入数组\n",
    "def getFilelist(path):\n",
    "    txtNames = []                     #所有txt文档名字\n",
    "    pathDir = os.listdir(path) #把文件夹里的所有txt文档的名字都获取了，放入数组\n",
    "    for allDir in pathDir:\n",
    "        if(allDir.find('txt') >= 0 and allDir.find('result') < 0):\n",
    "            txtNames.append(allDir)\n",
    "    return txtNames \n",
    "\n",
    "#具体的分词程序\n",
    "def fenci(filePath,filename,stop_single_words) :\n",
    "    f = open(filePath+'/'+filename,'r+',encoding='utf-8')\n",
    "    file_list = f.read()\n",
    "    f.close()\n",
    "    #正则化必须去除字母数字百分数，不然计算的TF-IDF在用来求提案质心时会出错\n",
    "    #错误类型：ValueError: 'GDP' is not in list\n",
    "    file_list=re.sub('[A-Za-z0-9,\\d+(\\.\\d+)?\\%]','',file_list) #去除字母,数字,百分数\n",
    "    #file_list=re.sub('[0-9,\\d+(\\.\\d+)?\\%]','',file_list)\n",
    "    \n",
    "    lis=[] #保存分词结果\n",
    "    seg_list = jieba.cut(file_list,cut_all=False)\n",
    "    for seg in seg_list : \n",
    "        seg = ''.join(seg.split())\n",
    "        if seg not in stop_single_words and len(seg)!=1: #为什么计算质心的时候不能有单字词\n",
    "            lis.append(seg)  \n",
    "    return lis\n",
    "\n",
    "#保存每篇提案的分词结果\n",
    "def save_word_seg_result(filename,word_seg_result,save_word_seg_path):\n",
    "    f = open(save_word_seg_path+\"/\"+filename,\"w+\",encoding='utf-8')\n",
    "    f.write(' '.join(word_seg_result))\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "#对标题分词\n",
    "def title_Participle(filePath,save_word_seg_path,stop_single_words):   \n",
    "    allfile = getFilelist(filePath) #获取所有提案的文件名\n",
    "    for file in allfile:\n",
    "        word_seg_result=fenci(filePath,file,stop_single_words)  #对每个提案分词\n",
    "        save_word_seg_result(file,word_seg_result,save_word_seg_path) \n",
    "    return\n",
    "        \n",
    "#对整个文档分词\n",
    "def Participle(filePath,save_word_seg_path,stop_single_words):   \n",
    "    word_count={}    #存放所有提案的 词-数量 键值对\n",
    "    reserved_words=[] #存放用于计算质心的词，也就是用于训练词向量的词\n",
    "    doc_name_words={} #保存所有提案的 名字-分词结果 键值对\n",
    "    sentences=[] #嵌套列表，保存每篇提案的分词结果\n",
    "    allfile = getFilelist(filePath) #获取所有提案的文件名\n",
    "    for file in allfile:\n",
    "        word_seg_result=fenci(filePath,file,stop_single_words)  #对每个提案分词\n",
    "        sentences.append(word_seg_result)\n",
    "        sentences.append(word_seg_result)\n",
    "        sentences.append(word_seg_result)\n",
    "        sentences.append(word_seg_result)\n",
    "        sentences.append(word_seg_result)\n",
    "        for word in word_seg_result:  #统计所有提案中每个词的数量\n",
    "            if word not in word_count.keys():\n",
    "                word_count[word]=1\n",
    "            else:\n",
    "                word_count[word]+=1\n",
    "        doc_name_words[file]=word_seg_result #将提案名和词按字典保存\n",
    "    doc_name_words_new={}\n",
    "    for key in doc_name_words.keys():\n",
    "        word_list=[]\n",
    "        for word_ in doc_name_words[key]:\n",
    "            if word_count[word_]>1:\n",
    "                word_list.append(word_)\n",
    "        #保存分词结果，平时注释掉\n",
    "        save_word_seg_result(key,word_list,save_word_seg_path) \n",
    "        doc_name_words_new[key]=word_list\n",
    "    for word in word_count.keys(): \n",
    "        if word_count[word]>1:\n",
    "            reserved_words.append(word)\n",
    "    return doc_name_words_new,reserved_words,sentences,allfile\n",
    "\n",
    "#训练词向量\n",
    "def train_vec(sentences,dimen):\n",
    "    model = word2vec.Word2Vec(sentences,min_count=2,size=dimen) #训练词向量\n",
    "    return model\n",
    "\n",
    "#读取800份已分词好的文档，进行TF-IDF计算。\n",
    "#这里是去除停用词的所有词，且删除了出现次数小于某一阈值的词\n",
    "def Tfidf(path,filelist):\n",
    "    corpus = []  #存取800份文档的分词结果\n",
    "    for ff in filelist :\n",
    "        fname = path +\"/\"+ff\n",
    "        f = open(fname,\"r+\",encoding=\"utf-8\")\n",
    "        content = f.read()\n",
    "        corpus.append(content)\n",
    "        f.close()\n",
    "\n",
    "    vectorizer = CountVectorizer()    \n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "    tfidf_word = vectorizer.get_feature_names() #所有文本的分词词语\n",
    "    tfidf_weight = tfidf.toarray()              #对应的tfidf矩阵\n",
    "    return tfidf_word,tfidf_weight.tolist()\n",
    "\n",
    "#计算每篇文档的质心\n",
    "#doc_name_words中的词删除了停用词和小于某一阈值（出现次数小于1）的词\n",
    "#reserved_words中的词删除了停用词和出现次数小于某一阈值的词\n",
    "def centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,threshold,title_weight,title_seg_lis,dimen):\n",
    "    doc_vec_list=[]\n",
    "    doc_vec_dic={}\n",
    "    for name in doc_name_words.keys():\n",
    "        sum_value=np.mat(zeros((1,dimen)))\n",
    "        count=0\n",
    "        for word in doc_name_words[name]:\n",
    "            weight=tfidf_weight[allfile.index(name)][tfidf_word.index(word)]\n",
    "            if weight>=threshold:\n",
    "                count+=1\n",
    "                if word in title_seg_lis:\n",
    "                    sum_value+=np.mat(model[word])*weight*title_weight#词向量*权重\n",
    "                else:\n",
    "                    sum_value+=np.mat(model[word])*weight #词向量*权重\n",
    "            #print(tfidf_weight[allfile.index(name)][tfidf_word.index(word)])\n",
    "        doc_vec_list.append(((sum_value/count).tolist())[0]) #这两处必须转化为列表，如果是数组，后面聚类程序保存文件的循环出错\n",
    "        doc_vec_dic[name]=((sum_value/count).tolist())[0]\n",
    "    return doc_vec_list,doc_vec_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#调参确定簇的个数\n",
    "def cluster_num(doc_vec):\n",
    "    \n",
    "    radius_ave_list=[]\n",
    "    diameter_ave_list=[]\n",
    "    n_clusters_list=[]\n",
    "    n_clusters_slope=[]\n",
    "    contour_coefficient_list=[]\n",
    "    \n",
    "    for n_clusters in range(3,25):\n",
    "        if n_clusters!=3:\n",
    "            n_clusters_slope.append(n_clusters)\n",
    "        n_clusters_list.append(n_clusters)\n",
    "        radius_ave,diameter_ave=select_cluster_num(doc_vec,n_clusters)\n",
    "        radius_ave_list.append(radius_ave)\n",
    "        diameter_ave_list.append(diameter_ave)\n",
    "        \n",
    "        contour_coefficient=contour_coefficient_Calculation(doc_vec,n_clusters)\n",
    "        contour_coefficient_list.append(contour_coefficient)\n",
    "    \n",
    "    #半径/直径的变化率\n",
    "    radius_rate_change_list=[]\n",
    "    diameter_rate_change_list=[]\n",
    "    for i in range(len(n_clusters_list)-1):\n",
    "        radius_rate_change=abs(radius_ave_list[i]-radius_ave_list[i+1])\n",
    "        radius_rate_change_list.append(radius_rate_change)\n",
    "        diameter_rate_change=abs(diameter_ave_list[i]-diameter_ave_list[i+1])\n",
    "        diameter_rate_change_list.append(diameter_rate_change)\n",
    "        \n",
    "    draw_clusters_change(n_clusters_list,radius_ave_list,\"The average radius varies with the number of clusters\",\"average radius\",\"the number of cluster\")\n",
    "    draw_clusters_change(n_clusters_list,diameter_ave_list,\"The average diameter varies with the number of clusters\",\"average diameter\",\"the number of cluster\")\n",
    "    \n",
    "    #draw_clusters_change_smooth(n_clusters_list,radius_ave_list,\"The average radius varies with the number of clusters\",\"average radius\")\n",
    "    #draw_clusters_change_smooth(n_clusters_list,diameter_ave_list,\"The average diameter varies with the number of clusters\",\"average diameter\")\n",
    "    \n",
    "#     draw_clusters_change(n_clusters_slope,radius_rate_change_list,\"Rate of change of average radius with number of clusters\",\"Rate of change of average radius\",\"the number of cluster\")\n",
    "#     draw_clusters_change(n_clusters_slope,diameter_rate_change_list,\"Rate of change of average diameter with number of clusters\",\"Rate of change of average diameter\",\"the number of cluster\")\n",
    "    \n",
    "    draw_clusters_change(n_clusters_list,contour_coefficient_list,\"The contour coefficient varies with the number of clusters\",\"contour coefficient\",\"the number of cluster\")\n",
    "    \n",
    "    #return n_clusters_list,radius_ave_list,diameter_ave_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算每个簇的平均半径和平均直径\n",
    "def select_cluster_num(doc_vec,n_clusters):\n",
    "    clustering_result={}\n",
    "    \n",
    "    estimator = KMeans(n_clusters)#构造聚类器\n",
    "    estimator.fit(doc_vec)#聚类\n",
    "    label_pred = estimator.labels_ #获取聚类标签    \n",
    "    \n",
    "    #将每一类放入对应的列表中\n",
    "    for i in range(len(label_pred)): #对于每一个聚类标签\n",
    "        for j in range(n_clusters): #判断属于哪一类\n",
    "            if label_pred[i]==j:\n",
    "                if j in clustering_result.keys():\n",
    "                    clustering_result[j].append(doc_vec[i])\n",
    "                else:\n",
    "                    clustering_result[j]=[doc_vec[i]] #键clustering_result[j]的值是一个嵌套列表\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    #计算簇的平均半径\n",
    "    radius_sum=0     #所有簇的半径和初始化\n",
    "    for name in clustering_result.keys():\n",
    "        radius_init=-1   #簇的半径初始化\n",
    "        init_vec=np.mat(zeros([len(clustering_result[name][0])]))\n",
    "        for vec in clustering_result[name]:\n",
    "            init_vec+=np.mat(vec)\n",
    "        centroid=init_vec/len(clustering_result[name]) #计算每个簇的质心\n",
    "        for vec in clustering_result[name]:\n",
    "            dist = np.sqrt(np.sum(np.square(np.mat(vec)-centroid)))\n",
    "            if dist>radius_init: #计算簇的半径\n",
    "                radius_init=dist\n",
    "        radius_sum+=radius_init  #所有簇的半径求和\n",
    "    radius_ave=radius_sum/n_clusters #平均半径\n",
    "    \n",
    "    #计算簇的平均直径\n",
    "    diameter_sum=0\n",
    "    for name in clustering_result.keys():\n",
    "        diameter_init=-1\n",
    "        for vec1 in clustering_result[name]:\n",
    "            for vec2 in clustering_result[name]:\n",
    "                dist = np.sqrt(np.sum(np.square(np.mat(vec1)-np.mat(vec2))))\n",
    "                if dist>diameter_init:\n",
    "                    diameter_init=dist\n",
    "        diameter_sum+=diameter_init\n",
    "    diameter_ave=diameter_sum/n_clusters\n",
    "    \n",
    "    return radius_ave,diameter_ave\n",
    "\n",
    "#聚类\n",
    "def kmeans_cluster(doc_vec_list,doc_vec_dic,n_clusters):\n",
    "    clustering_result={}\n",
    "    \n",
    "    estimator = KMeans(n_clusters)#构造聚类器\n",
    "    estimator.fit(doc_vec_list)#聚类\n",
    "    label_pred = estimator.labels_ #获取聚类标签    \n",
    "    \n",
    "    name_list=[]\n",
    "    vec_list=[]\n",
    "    for key,value in doc_vec_dic.items( ):\n",
    "        name_list.append(key)\n",
    "        vec_list.append(value)\n",
    "        \n",
    "    #将每一类放入对应的列表中\n",
    "    for i in range(len(label_pred)): #对于每一个聚类标签\n",
    "        for j in range(n_clusters): #判断属于哪一类\n",
    "            if label_pred[i]==j:\n",
    "                if j in clustering_result.keys():\n",
    "                    clustering_result[j].append(name_list[vec_list.index(doc_vec_list[i])])\n",
    "                else:\n",
    "                    clustering_result[j]=[name_list[vec_list.index(doc_vec_list[i])]] #键clustering_result[j]的值是一个嵌套列表\n",
    "            else:\n",
    "                pass\n",
    "    return clustering_result\n",
    "\n",
    "#将每个簇对应的文档保存到相应的文件夹下\n",
    "def doc_save(clustering_result,filePath,savepath,n_clusters):\n",
    "\n",
    "    for i in os.listdir(savepath):\n",
    "        if i!='.DS_Store':\n",
    "            path_file = os.path.join(savepath,i)  #取文件路径\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "            for f in os.listdir(path_file): \n",
    "                if f!='.DS_Store':\n",
    "                    path_file2 =os.path.join(path_file,f)\n",
    "                    if os.path.isfile(path_file2):\n",
    "                        os.remove(path_file2)\n",
    "                    \n",
    "    for i in clustering_result.keys():\n",
    "        for j in clustering_result[i]:\n",
    "            f1 = open(filePath+\"/\"+j,\"r+\",encoding=\"utf-8\")\n",
    "            file_list = f1.read()\n",
    "            f1.close()\n",
    "            f2 = open(savepath+\"/\"+str(i+1)+\".txt\"+\"/\"+j,\"w+\",encoding=\"utf-8\")\n",
    "            f2.write(file_list)\n",
    "            f2.close()\n",
    "    #输出每一类的提案个数\n",
    "    for i in range(n_clusters):\n",
    "        print(\"主题\"+str(i+1)+\"的提案个数：\",len(clustering_result[i]))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算轮廓系数\n",
    "def contour_coefficient_Calculation(doc_vec,n_clusters):\n",
    "    clustering_result={}\n",
    "    \n",
    "    estimator = KMeans(n_clusters)#构造聚类器\n",
    "    estimator.fit(doc_vec)#聚类\n",
    "    label_pred = estimator.labels_ #获取聚类标签   \n",
    "    \n",
    "    #计算簇的轮廓系数\n",
    "    contour_coefficient=metrics.silhouette_score(doc_vec, label_pred, metric='euclidean')\n",
    "    \n",
    "    return contour_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#调参确定保留多少词求每篇提案的质心\n",
    "def centroid_word(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,title_seg_lis,dimen):\n",
    "    contour_coefficient_list=[] #轮廓系数列表\n",
    "    words_Threshold=[0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]\n",
    "    for threshold in words_Threshold:\n",
    "        doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,threshold,8,title_seg_lis,dimen)\n",
    "        contour_coefficient=contour_coefficient_Calculation(doc_vec_list,10)\n",
    "        contour_coefficient_list.append(contour_coefficient)\n",
    "        print(threshold)\n",
    "    draw_clusters_change(words_Threshold,contour_coefficient_list,\"Contour coefficient changes with the number of words retained\",\"contour coefficient\",\"the number of words retained\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#调参确定标题正文的权重比\n",
    "def title_text(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,title_seg_lis,dimen):\n",
    "    contour_coefficient_list=[] #轮廓系数列表\n",
    "    weight_ratio=[1,2,3,4,5,6,7,8]\n",
    "    for title_weight in weight_ratio:\n",
    "        doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,0.08,title_weight,title_seg_lis,dimen)\n",
    "        contour_coefficient=contour_coefficient_Calculation(doc_vec_list,14)\n",
    "        contour_coefficient_list.append(contour_coefficient)\n",
    "        print(title_weight)\n",
    "    draw_clusters_change(weight_ratio,contour_coefficient_list,\"Contour coefficient changes with the weight ratio of the title and body\",\"contour coefficient\",\"Weight ratio of title and body\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_clusters_change(n_clusters_list,evaluation_index,title,ylabel,xlabel):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(n_clusters_list, evaluation_index,'ro')\n",
    "    ax.plot(n_clusters_list, evaluation_index)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def draw_clusters_change_smooth(n_clusters_list,evaluation_index,title,ylabel,xlabel):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    n_clusters_list_new = np.linspace(min(n_clusters_list),max(n_clusters_list),300) #300 represents number of points to make between T.min and T.max\n",
    "    evaluation_index_smooth = spline(n_clusters_list,evaluation_index,n_clusters_list_new)\n",
    "    ax.plot(n_clusters_list_new, evaluation_index_smooth,'r')\n",
    "    ax.plot(n_clusters_list_new, evaluation_index_smooth)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#提取聚类结果中同一个簇下的文档的关键词，将同一个词的TF-IDF相加，作为这个词的权重，选取权重最高的词作为这个簇的关键词\n",
    "def key_word_cluster_TF_merge(savepath,key_words_savepath,tfidf_word,tfidf_weight,allfile):\n",
    "    basic_keyword_num=3\n",
    "    dic={}\n",
    "    #循环savepath下文件夹中的每个子文件夹\n",
    "    for i in os.listdir(savepath):\n",
    "        if i!='.DS_Store':\n",
    "            path_file = os.path.join(savepath,i)  #取子文件夹路径\n",
    "            \n",
    "            word_top=[] #存放该簇下的关键词\n",
    "            word_weight={} #将该簇中的同一个词按照权重加起来\n",
    "            \n",
    "            #将同一个簇中的文档关键词按照TF-IDF取前keynum个，并存入word_top列表中\n",
    "            cluster_files=getFilelist(path_file)\n",
    "            for file in cluster_files:\n",
    "                weight_index=allfile.index(file) #该篇文档所在的weight行\n",
    "                for j in range(len(tfidf_word)):\n",
    "                    if len(tfidf_word[j])>1:\n",
    "                        if tfidf_word[j]!='北京' and tfidf_word[j]!='北京市':\n",
    "                            if tfidf_word[j] not in word_weight.keys():\n",
    "                                word_weight[tfidf_word[j]]=tfidf_weight[weight_index][j]\n",
    "                            else:\n",
    "                                word_weight[tfidf_word[j]]+=tfidf_weight[weight_index][j]\n",
    "            word_weight=sorted(word_weight.items(),key=lambda A:A[1],reverse=True)\n",
    "            keyword_num=basic_keyword_num*len(cluster_files)\n",
    "            for k in word_weight[:keyword_num]: #取该簇中权重最大的前keynum个词\n",
    "                word_top.append(k[0])\n",
    "            if i=='1.txt':\n",
    "                dic[1]=word_top[:3]\n",
    "            if i=='2.txt':\n",
    "                dic[2]=word_top[:3]\n",
    "            if i=='3.txt':\n",
    "                dic[3]=word_top[:3]\n",
    "            if i=='4.txt':\n",
    "                dic[4]=word_top[:3]\n",
    "            if i=='5.txt':\n",
    "                dic[5]=word_top[:3]\n",
    "            if i=='6.txt':\n",
    "                dic[6]=word_top[:3]\n",
    "            if i=='7.txt':\n",
    "                dic[7]=word_top[:3]\n",
    "            if i=='8.txt':\n",
    "                dic[8]=word_top[:3]\n",
    "            #print(str(i),\"  \",word_top[:3])\n",
    "            \n",
    "            f = open(key_words_savepath+\"/\"+str(i),\"w+\",encoding=\"utf-8\")\n",
    "            f.write(\" \".join(word_top))\n",
    "            f.close()\n",
    "            \n",
    "    for iii in sorted(dic.items(),key = lambda x:x[0]):\n",
    "        print(\"主题\"+str(iii[0])+\"的关键词:\",iii[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:123: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Applications/anaconda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:125: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题1的提案个数： 65\n",
      "主题2的提案个数： 83\n",
      "主题3的提案个数： 406\n",
      "主题4的提案个数： 59\n",
      "主题5的提案个数： 100\n",
      "主题6的提案个数： 15\n",
      "主题7的提案个数： 67\n",
      "主题8的提案个数： 3\n",
      "主题1的关键词: ['文化', '保护', '文化遗产']\n",
      "主题2的关键词: ['产业', '企业', '创新']\n",
      "主题3的关键词: ['建设', '城市', '发展']\n",
      "主题4的关键词: ['教育', '教师', '学生']\n",
      "主题5的关键词: ['停车', '车辆', '机动车']\n",
      "主题6的关键词: ['垃圾', '垃圾分类', '分类']\n",
      "主题7的关键词: ['服务', '社区', '医疗']\n",
      "主题8的关键词: ['污泥', '资源化', '处理']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" : \n",
    "    stopwords=\"/Users/lyj/Desktop/提案程序/百度停用词列表（去除英文）+tycb_symbol.txt\"  \n",
    "    filePath = \"/Users/lyj/Desktop/提案程序/提案数据\"  #txt文本集所在的文件夹路径\n",
    "    save_word_seg_path=\"/Users/lyj/Desktop/提案程序/zxtafenci\" #保存分词结果的路径\n",
    "    title_path=\"/Users/lyj/Desktop/提案程序\" #所有标题所在的路径（所有标题在一个txt文档中）\n",
    "    savepath=\"/Users/lyj/Desktop/提案程序/word2veccluster\" #保存聚类结果\n",
    "    title_save_word_seg_path='/Users/lyj/Desktop/提案程序/标题分词' #保存标题的分词结果\n",
    "    key_words_savepath=\"/Users/lyj/Desktop/提案程序/候选关键词\"\n",
    "\n",
    "    stop_single_words=readstopwords(stopwords)\n",
    "    title_seg_lis=list(set(fenci(title_path,'title.txt',stop_single_words))) #对标题分词 \n",
    "    doc_name_words,reserved_words,sentences,allfile=Participle(filePath,save_word_seg_path,stop_single_words) #对整个文档分词\n",
    "    tfidf_word,tfidf_weight=Tfidf(save_word_seg_path,allfile)\n",
    "    \n",
    "    dimen=300 #词向量维度\n",
    "    model=train_vec(sentences,dimen)\n",
    "    #求每篇文档的质心\n",
    "    #这是一条说明语句doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,threshold,weight,title_seg_lis)\n",
    "    doc_vec_list,doc_vec_dic=centroid_TF(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,0.04,5,title_seg_lis,dimen)\n",
    "    \n",
    "    #调参确定各个参数\n",
    "    #cluster_num(doc_vec_list) #确定簇的个数\n",
    "    #centroid_word(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,title_seg_lis,dimen) #确定保留的词数\n",
    "    #title_text(doc_name_words,reserved_words,model,allfile,tfidf_word,tfidf_weight,title_seg_lis,dimen) #确定标题-正文权重比\n",
    "    \n",
    "    \n",
    "    #聚类，并将聚类结果保存，计算轮廓系数\n",
    "    clustering_result=kmeans_cluster(doc_vec_list,doc_vec_dic,8)\n",
    "    doc_save(clustering_result,filePath,savepath,8)\n",
    "    contour_coefficient=contour_coefficient_Calculation(doc_vec_list,8)\n",
    "    #print(contour_coefficient)\n",
    "        \n",
    "    #将聚类结果中，同一个簇中的文档中的同一个词的TF-IDF值相加提取关键词\n",
    "    key_word_cluster_TF_merge(savepath,key_words_savepath,tfidf_word,tfidf_weight,allfile)\n",
    "    #print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
