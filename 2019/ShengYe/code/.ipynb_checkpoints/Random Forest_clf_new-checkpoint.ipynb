{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿userid</th>\n",
       "      <th>original_weibo</th>\n",
       "      <th>repost_weibo</th>\n",
       "      <th>like_weibo</th>\n",
       "      <th>follow</th>\n",
       "      <th>fans</th>\n",
       "      <th>vip_level</th>\n",
       "      <th>av_comment</th>\n",
       "      <th>av_like</th>\n",
       "      <th>av_re_comment</th>\n",
       "      <th>...</th>\n",
       "      <th>product_comm</th>\n",
       "      <th>product_comm_sent</th>\n",
       "      <th>update</th>\n",
       "      <th>neg_com_perc</th>\n",
       "      <th>turnover_month_appr</th>\n",
       "      <th>sales_month</th>\n",
       "      <th>credit</th>\n",
       "      <th>refund_rate</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>com_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2834</td>\n",
       "      <td>679</td>\n",
       "      <td>1343</td>\n",
       "      <td>316</td>\n",
       "      <td>3775598</td>\n",
       "      <td>45</td>\n",
       "      <td>703.142214</td>\n",
       "      <td>1150.808223</td>\n",
       "      <td>3551.350806</td>\n",
       "      <td>...</td>\n",
       "      <td>21346.00</td>\n",
       "      <td>1.013699</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041571</td>\n",
       "      <td>1.914995e+06</td>\n",
       "      <td>2778</td>\n",
       "      <td>893935</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3333</td>\n",
       "      <td>0.986199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4082</td>\n",
       "      <td>1164</td>\n",
       "      <td>1062</td>\n",
       "      <td>393</td>\n",
       "      <td>4348114</td>\n",
       "      <td>48</td>\n",
       "      <td>83.097712</td>\n",
       "      <td>396.988982</td>\n",
       "      <td>1060.471216</td>\n",
       "      <td>...</td>\n",
       "      <td>81867.00</td>\n",
       "      <td>0.875705</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032215</td>\n",
       "      <td>1.797427e+07</td>\n",
       "      <td>54238</td>\n",
       "      <td>826952</td>\n",
       "      <td>0.55</td>\n",
       "      <td>22529</td>\n",
       "      <td>0.998003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4050</td>\n",
       "      <td>2285</td>\n",
       "      <td>538</td>\n",
       "      <td>751</td>\n",
       "      <td>778999</td>\n",
       "      <td>48</td>\n",
       "      <td>9.862074</td>\n",
       "      <td>248.611844</td>\n",
       "      <td>114.159870</td>\n",
       "      <td>...</td>\n",
       "      <td>124781.00</td>\n",
       "      <td>0.906117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055203</td>\n",
       "      <td>1.558100e+07</td>\n",
       "      <td>92442</td>\n",
       "      <td>2288782</td>\n",
       "      <td>0.32</td>\n",
       "      <td>40605</td>\n",
       "      <td>0.996725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1351</td>\n",
       "      <td>766</td>\n",
       "      <td>262</td>\n",
       "      <td>334</td>\n",
       "      <td>1241240</td>\n",
       "      <td>43</td>\n",
       "      <td>257.978234</td>\n",
       "      <td>2177.106605</td>\n",
       "      <td>370.986234</td>\n",
       "      <td>...</td>\n",
       "      <td>309598.00</td>\n",
       "      <td>1.018530</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051287</td>\n",
       "      <td>1.654340e+07</td>\n",
       "      <td>121452</td>\n",
       "      <td>4169378</td>\n",
       "      <td>0.41</td>\n",
       "      <td>58071</td>\n",
       "      <td>0.998640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>3772</td>\n",
       "      <td>1145</td>\n",
       "      <td>1917</td>\n",
       "      <td>701</td>\n",
       "      <td>2498820</td>\n",
       "      <td>48</td>\n",
       "      <td>733.688423</td>\n",
       "      <td>996.604790</td>\n",
       "      <td>1113.772453</td>\n",
       "      <td>...</td>\n",
       "      <td>25911.00</td>\n",
       "      <td>0.846137</td>\n",
       "      <td>1</td>\n",
       "      <td>0.049384</td>\n",
       "      <td>3.546000e+06</td>\n",
       "      <td>20365</td>\n",
       "      <td>945335</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12194</td>\n",
       "      <td>0.989339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2457</td>\n",
       "      <td>1126</td>\n",
       "      <td>696</td>\n",
       "      <td>442</td>\n",
       "      <td>3270155</td>\n",
       "      <td>43</td>\n",
       "      <td>221.039438</td>\n",
       "      <td>460.953958</td>\n",
       "      <td>234.175818</td>\n",
       "      <td>...</td>\n",
       "      <td>33467.00</td>\n",
       "      <td>0.918922</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044797</td>\n",
       "      <td>1.395485e+07</td>\n",
       "      <td>96870</td>\n",
       "      <td>1635805</td>\n",
       "      <td>0.41</td>\n",
       "      <td>17517</td>\n",
       "      <td>0.992750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>3605</td>\n",
       "      <td>1438</td>\n",
       "      <td>120</td>\n",
       "      <td>759</td>\n",
       "      <td>2295703</td>\n",
       "      <td>48</td>\n",
       "      <td>117.579598</td>\n",
       "      <td>262.434421</td>\n",
       "      <td>334.918314</td>\n",
       "      <td>...</td>\n",
       "      <td>45051.00</td>\n",
       "      <td>0.922688</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>6.610076e+06</td>\n",
       "      <td>26396</td>\n",
       "      <td>1278608</td>\n",
       "      <td>0.82</td>\n",
       "      <td>17076</td>\n",
       "      <td>0.997599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>2199</td>\n",
       "      <td>211</td>\n",
       "      <td>272</td>\n",
       "      <td>181</td>\n",
       "      <td>5733434</td>\n",
       "      <td>48</td>\n",
       "      <td>3142.379453</td>\n",
       "      <td>7556.855960</td>\n",
       "      <td>2279.682713</td>\n",
       "      <td>...</td>\n",
       "      <td>334594.00</td>\n",
       "      <td>1.206648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>3.598865e+07</td>\n",
       "      <td>237754</td>\n",
       "      <td>2317829</td>\n",
       "      <td>0.38</td>\n",
       "      <td>154830</td>\n",
       "      <td>0.997462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>1369</td>\n",
       "      <td>515</td>\n",
       "      <td>2318</td>\n",
       "      <td>183</td>\n",
       "      <td>2879664</td>\n",
       "      <td>48</td>\n",
       "      <td>1831.429379</td>\n",
       "      <td>865.555727</td>\n",
       "      <td>2986.545552</td>\n",
       "      <td>...</td>\n",
       "      <td>139566.00</td>\n",
       "      <td>0.989010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046109</td>\n",
       "      <td>2.506724e+06</td>\n",
       "      <td>15913</td>\n",
       "      <td>2435007</td>\n",
       "      <td>0.91</td>\n",
       "      <td>74413</td>\n",
       "      <td>0.996210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>2025</td>\n",
       "      <td>477</td>\n",
       "      <td>928</td>\n",
       "      <td>567</td>\n",
       "      <td>502984</td>\n",
       "      <td>46</td>\n",
       "      <td>188.166470</td>\n",
       "      <td>988.913420</td>\n",
       "      <td>1336.992481</td>\n",
       "      <td>...</td>\n",
       "      <td>97986.00</td>\n",
       "      <td>0.926261</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062744</td>\n",
       "      <td>1.879275e+07</td>\n",
       "      <td>64978</td>\n",
       "      <td>1195232</td>\n",
       "      <td>0.40</td>\n",
       "      <td>36685</td>\n",
       "      <td>0.998446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>2850</td>\n",
       "      <td>245</td>\n",
       "      <td>1952</td>\n",
       "      <td>147</td>\n",
       "      <td>387598</td>\n",
       "      <td>41</td>\n",
       "      <td>866.134926</td>\n",
       "      <td>654.452550</td>\n",
       "      <td>1631.643773</td>\n",
       "      <td>...</td>\n",
       "      <td>304884.00</td>\n",
       "      <td>1.009798</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053261</td>\n",
       "      <td>3.006963e+07</td>\n",
       "      <td>168148</td>\n",
       "      <td>1896732</td>\n",
       "      <td>0.53</td>\n",
       "      <td>124381</td>\n",
       "      <td>0.997339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>3005</td>\n",
       "      <td>586</td>\n",
       "      <td>818</td>\n",
       "      <td>768</td>\n",
       "      <td>1358200</td>\n",
       "      <td>44</td>\n",
       "      <td>360.139734</td>\n",
       "      <td>171.499447</td>\n",
       "      <td>835.014001</td>\n",
       "      <td>...</td>\n",
       "      <td>27745.00</td>\n",
       "      <td>0.761522</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041215</td>\n",
       "      <td>1.357244e+06</td>\n",
       "      <td>6330</td>\n",
       "      <td>192088</td>\n",
       "      <td>1.19</td>\n",
       "      <td>4309</td>\n",
       "      <td>0.996751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>2120</td>\n",
       "      <td>408</td>\n",
       "      <td>926</td>\n",
       "      <td>695</td>\n",
       "      <td>4095687</td>\n",
       "      <td>48</td>\n",
       "      <td>1999.103931</td>\n",
       "      <td>4214.670689</td>\n",
       "      <td>2023.294770</td>\n",
       "      <td>...</td>\n",
       "      <td>166073.00</td>\n",
       "      <td>1.141223</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>3.654729e+07</td>\n",
       "      <td>282725</td>\n",
       "      <td>4380269</td>\n",
       "      <td>0.45</td>\n",
       "      <td>198728</td>\n",
       "      <td>0.996820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>3238</td>\n",
       "      <td>1007</td>\n",
       "      <td>993</td>\n",
       "      <td>615</td>\n",
       "      <td>2775128</td>\n",
       "      <td>45</td>\n",
       "      <td>410.918386</td>\n",
       "      <td>329.528171</td>\n",
       "      <td>571.432974</td>\n",
       "      <td>...</td>\n",
       "      <td>497827.00</td>\n",
       "      <td>1.001447</td>\n",
       "      <td>1</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>2.592427e+07</td>\n",
       "      <td>156271</td>\n",
       "      <td>3729398</td>\n",
       "      <td>0.50</td>\n",
       "      <td>55984</td>\n",
       "      <td>0.994338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>7514</td>\n",
       "      <td>3470</td>\n",
       "      <td>2735</td>\n",
       "      <td>458</td>\n",
       "      <td>84325</td>\n",
       "      <td>41</td>\n",
       "      <td>587.342612</td>\n",
       "      <td>677.125547</td>\n",
       "      <td>1144.501976</td>\n",
       "      <td>...</td>\n",
       "      <td>810179.00</td>\n",
       "      <td>1.134226</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>5.748386e+07</td>\n",
       "      <td>303797</td>\n",
       "      <td>3405924</td>\n",
       "      <td>0.44</td>\n",
       "      <td>134206</td>\n",
       "      <td>0.999076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>2006</td>\n",
       "      <td>530</td>\n",
       "      <td>12</td>\n",
       "      <td>509</td>\n",
       "      <td>346674</td>\n",
       "      <td>47</td>\n",
       "      <td>281.138243</td>\n",
       "      <td>538.023997</td>\n",
       "      <td>537.522484</td>\n",
       "      <td>...</td>\n",
       "      <td>40058.00</td>\n",
       "      <td>0.744758</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056316</td>\n",
       "      <td>1.316547e+07</td>\n",
       "      <td>78516</td>\n",
       "      <td>1989581</td>\n",
       "      <td>0.73</td>\n",
       "      <td>9419</td>\n",
       "      <td>0.993205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>2572</td>\n",
       "      <td>2398</td>\n",
       "      <td>2434</td>\n",
       "      <td>281</td>\n",
       "      <td>2010240</td>\n",
       "      <td>45</td>\n",
       "      <td>70.314722</td>\n",
       "      <td>1402.445561</td>\n",
       "      <td>397.879566</td>\n",
       "      <td>...</td>\n",
       "      <td>639770.00</td>\n",
       "      <td>1.051818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.064702</td>\n",
       "      <td>1.448977e+08</td>\n",
       "      <td>307505</td>\n",
       "      <td>2607424</td>\n",
       "      <td>0.56</td>\n",
       "      <td>29162</td>\n",
       "      <td>0.990810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>1970</td>\n",
       "      <td>937</td>\n",
       "      <td>617</td>\n",
       "      <td>182</td>\n",
       "      <td>137185</td>\n",
       "      <td>48</td>\n",
       "      <td>303.773913</td>\n",
       "      <td>239.317848</td>\n",
       "      <td>214.834958</td>\n",
       "      <td>...</td>\n",
       "      <td>12026.00</td>\n",
       "      <td>0.917028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043321</td>\n",
       "      <td>1.755535e+07</td>\n",
       "      <td>30511</td>\n",
       "      <td>451809</td>\n",
       "      <td>0.49</td>\n",
       "      <td>4147</td>\n",
       "      <td>0.996383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>1316</td>\n",
       "      <td>142</td>\n",
       "      <td>987</td>\n",
       "      <td>10</td>\n",
       "      <td>67843</td>\n",
       "      <td>43</td>\n",
       "      <td>213.211853</td>\n",
       "      <td>105.755450</td>\n",
       "      <td>1112.059783</td>\n",
       "      <td>...</td>\n",
       "      <td>7167.00</td>\n",
       "      <td>0.712781</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>6.875260e+05</td>\n",
       "      <td>4700</td>\n",
       "      <td>139380</td>\n",
       "      <td>0.26</td>\n",
       "      <td>884</td>\n",
       "      <td>0.994344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23</td>\n",
       "      <td>589</td>\n",
       "      <td>244</td>\n",
       "      <td>100</td>\n",
       "      <td>277</td>\n",
       "      <td>11725</td>\n",
       "      <td>40</td>\n",
       "      <td>19.427746</td>\n",
       "      <td>271.838117</td>\n",
       "      <td>355.418398</td>\n",
       "      <td>...</td>\n",
       "      <td>15792.00</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071461</td>\n",
       "      <td>2.262100e+06</td>\n",
       "      <td>6838</td>\n",
       "      <td>314468</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1277</td>\n",
       "      <td>0.995301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>794</td>\n",
       "      <td>155</td>\n",
       "      <td>636</td>\n",
       "      <td>219</td>\n",
       "      <td>513576</td>\n",
       "      <td>46</td>\n",
       "      <td>1150.535975</td>\n",
       "      <td>879.478624</td>\n",
       "      <td>1962.353393</td>\n",
       "      <td>...</td>\n",
       "      <td>4209.00</td>\n",
       "      <td>0.393804</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033882</td>\n",
       "      <td>2.083316e+06</td>\n",
       "      <td>10522</td>\n",
       "      <td>236781</td>\n",
       "      <td>0.76</td>\n",
       "      <td>7049</td>\n",
       "      <td>0.996170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>2108</td>\n",
       "      <td>553</td>\n",
       "      <td>746</td>\n",
       "      <td>464</td>\n",
       "      <td>53265</td>\n",
       "      <td>48</td>\n",
       "      <td>167.710166</td>\n",
       "      <td>492.197412</td>\n",
       "      <td>263.559969</td>\n",
       "      <td>...</td>\n",
       "      <td>139167.00</td>\n",
       "      <td>0.808987</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>1.534274e+07</td>\n",
       "      <td>47315</td>\n",
       "      <td>820393</td>\n",
       "      <td>0.48</td>\n",
       "      <td>36022</td>\n",
       "      <td>0.998862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27</td>\n",
       "      <td>766</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>222</td>\n",
       "      <td>1393041</td>\n",
       "      <td>41</td>\n",
       "      <td>214.612469</td>\n",
       "      <td>106.442543</td>\n",
       "      <td>1602.450980</td>\n",
       "      <td>...</td>\n",
       "      <td>46192.00</td>\n",
       "      <td>0.891414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043740</td>\n",
       "      <td>3.049598e+06</td>\n",
       "      <td>18306</td>\n",
       "      <td>551169</td>\n",
       "      <td>0.27</td>\n",
       "      <td>9784</td>\n",
       "      <td>0.999285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28</td>\n",
       "      <td>1673</td>\n",
       "      <td>554</td>\n",
       "      <td>183</td>\n",
       "      <td>266</td>\n",
       "      <td>613758</td>\n",
       "      <td>42</td>\n",
       "      <td>131.358681</td>\n",
       "      <td>96.560000</td>\n",
       "      <td>1766.012676</td>\n",
       "      <td>...</td>\n",
       "      <td>2104.00</td>\n",
       "      <td>0.715268</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045230</td>\n",
       "      <td>2.831217e+05</td>\n",
       "      <td>1180</td>\n",
       "      <td>87509</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1275</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>1825</td>\n",
       "      <td>171</td>\n",
       "      <td>246</td>\n",
       "      <td>112</td>\n",
       "      <td>731378</td>\n",
       "      <td>45</td>\n",
       "      <td>2820.496210</td>\n",
       "      <td>2486.370055</td>\n",
       "      <td>1639.251244</td>\n",
       "      <td>...</td>\n",
       "      <td>219703.00</td>\n",
       "      <td>0.894031</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079447</td>\n",
       "      <td>1.207810e+07</td>\n",
       "      <td>93160</td>\n",
       "      <td>1040827</td>\n",
       "      <td>0.28</td>\n",
       "      <td>43586</td>\n",
       "      <td>0.998325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>397</td>\n",
       "      <td>151</td>\n",
       "      <td>236</td>\n",
       "      <td>965</td>\n",
       "      <td>160499</td>\n",
       "      <td>41</td>\n",
       "      <td>15.322183</td>\n",
       "      <td>4.103873</td>\n",
       "      <td>9491.859838</td>\n",
       "      <td>...</td>\n",
       "      <td>63145160.02</td>\n",
       "      <td>1.442884</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035580</td>\n",
       "      <td>3.157258e+07</td>\n",
       "      <td>326284</td>\n",
       "      <td>11289676</td>\n",
       "      <td>0.38</td>\n",
       "      <td>233716</td>\n",
       "      <td>0.990232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>301</td>\n",
       "      <td>368</td>\n",
       "      <td>237</td>\n",
       "      <td>729</td>\n",
       "      <td>476635</td>\n",
       "      <td>48</td>\n",
       "      <td>114.324693</td>\n",
       "      <td>179.536153</td>\n",
       "      <td>453.633613</td>\n",
       "      <td>...</td>\n",
       "      <td>18075.00</td>\n",
       "      <td>0.918081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>5.319155e+05</td>\n",
       "      <td>1031</td>\n",
       "      <td>231133</td>\n",
       "      <td>0.17</td>\n",
       "      <td>597</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>889</td>\n",
       "      <td>169</td>\n",
       "      <td>148</td>\n",
       "      <td>88</td>\n",
       "      <td>1701109</td>\n",
       "      <td>41</td>\n",
       "      <td>7.535014</td>\n",
       "      <td>11.290383</td>\n",
       "      <td>6316.531353</td>\n",
       "      <td>...</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>0.868668</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051687</td>\n",
       "      <td>1.100708e+05</td>\n",
       "      <td>502</td>\n",
       "      <td>118972</td>\n",
       "      <td>0.28</td>\n",
       "      <td>262</td>\n",
       "      <td>0.996183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>1255</td>\n",
       "      <td>591</td>\n",
       "      <td>4279</td>\n",
       "      <td>144</td>\n",
       "      <td>339578</td>\n",
       "      <td>43</td>\n",
       "      <td>135.165486</td>\n",
       "      <td>453.283907</td>\n",
       "      <td>6124.331541</td>\n",
       "      <td>...</td>\n",
       "      <td>147129.00</td>\n",
       "      <td>0.885049</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046152</td>\n",
       "      <td>6.757381e+06</td>\n",
       "      <td>38450</td>\n",
       "      <td>764369</td>\n",
       "      <td>0.28</td>\n",
       "      <td>46290</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37</td>\n",
       "      <td>606</td>\n",
       "      <td>28</td>\n",
       "      <td>408</td>\n",
       "      <td>215</td>\n",
       "      <td>169289</td>\n",
       "      <td>47</td>\n",
       "      <td>381.339090</td>\n",
       "      <td>356.163265</td>\n",
       "      <td>5331.381944</td>\n",
       "      <td>...</td>\n",
       "      <td>9742.00</td>\n",
       "      <td>0.539108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.061203</td>\n",
       "      <td>1.120556e+05</td>\n",
       "      <td>614</td>\n",
       "      <td>368959</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.998777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>73</td>\n",
       "      <td>785</td>\n",
       "      <td>126</td>\n",
       "      <td>70</td>\n",
       "      <td>582</td>\n",
       "      <td>966023</td>\n",
       "      <td>48</td>\n",
       "      <td>2805.534276</td>\n",
       "      <td>3327.886834</td>\n",
       "      <td>8834.175532</td>\n",
       "      <td>...</td>\n",
       "      <td>1295.00</td>\n",
       "      <td>0.917465</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030986</td>\n",
       "      <td>4.833896e+05</td>\n",
       "      <td>1725</td>\n",
       "      <td>6441</td>\n",
       "      <td>0.38</td>\n",
       "      <td>258</td>\n",
       "      <td>0.996124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>75</td>\n",
       "      <td>1690</td>\n",
       "      <td>123</td>\n",
       "      <td>4024</td>\n",
       "      <td>967</td>\n",
       "      <td>978416</td>\n",
       "      <td>48</td>\n",
       "      <td>3832.375138</td>\n",
       "      <td>16229.174370</td>\n",
       "      <td>3039.881455</td>\n",
       "      <td>...</td>\n",
       "      <td>17897.00</td>\n",
       "      <td>0.831589</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073037</td>\n",
       "      <td>9.024420e+05</td>\n",
       "      <td>9153</td>\n",
       "      <td>354968</td>\n",
       "      <td>0.23</td>\n",
       "      <td>14313</td>\n",
       "      <td>0.994481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>76</td>\n",
       "      <td>4030</td>\n",
       "      <td>1876</td>\n",
       "      <td>480</td>\n",
       "      <td>262</td>\n",
       "      <td>3660599</td>\n",
       "      <td>48</td>\n",
       "      <td>31.831060</td>\n",
       "      <td>67.575349</td>\n",
       "      <td>2248.821786</td>\n",
       "      <td>...</td>\n",
       "      <td>12861.00</td>\n",
       "      <td>0.521505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>2.048860e+06</td>\n",
       "      <td>2736</td>\n",
       "      <td>231472</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2415</td>\n",
       "      <td>0.997930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>77</td>\n",
       "      <td>3088</td>\n",
       "      <td>1539</td>\n",
       "      <td>380</td>\n",
       "      <td>370</td>\n",
       "      <td>1169908</td>\n",
       "      <td>46</td>\n",
       "      <td>54.197901</td>\n",
       "      <td>40.970884</td>\n",
       "      <td>1103.936782</td>\n",
       "      <td>...</td>\n",
       "      <td>41346.00</td>\n",
       "      <td>0.958541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064628</td>\n",
       "      <td>1.301925e+06</td>\n",
       "      <td>981</td>\n",
       "      <td>200038</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1296</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>79</td>\n",
       "      <td>5578</td>\n",
       "      <td>4167</td>\n",
       "      <td>1333</td>\n",
       "      <td>275</td>\n",
       "      <td>1719900</td>\n",
       "      <td>47</td>\n",
       "      <td>110.445045</td>\n",
       "      <td>48.684866</td>\n",
       "      <td>746.712513</td>\n",
       "      <td>...</td>\n",
       "      <td>26007.00</td>\n",
       "      <td>0.853030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112284</td>\n",
       "      <td>5.266697e+06</td>\n",
       "      <td>20087</td>\n",
       "      <td>322299</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3071</td>\n",
       "      <td>0.997069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>80</td>\n",
       "      <td>1105</td>\n",
       "      <td>52</td>\n",
       "      <td>625</td>\n",
       "      <td>218</td>\n",
       "      <td>250745</td>\n",
       "      <td>38</td>\n",
       "      <td>293.062772</td>\n",
       "      <td>1943.590517</td>\n",
       "      <td>559.653274</td>\n",
       "      <td>...</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>0.127260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>2.820965e+05</td>\n",
       "      <td>3125</td>\n",
       "      <td>118292</td>\n",
       "      <td>1.05</td>\n",
       "      <td>79</td>\n",
       "      <td>0.924051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>81</td>\n",
       "      <td>1627</td>\n",
       "      <td>457</td>\n",
       "      <td>472</td>\n",
       "      <td>897</td>\n",
       "      <td>1181707</td>\n",
       "      <td>48</td>\n",
       "      <td>741.445026</td>\n",
       "      <td>1428.446930</td>\n",
       "      <td>2259.100220</td>\n",
       "      <td>...</td>\n",
       "      <td>59902.00</td>\n",
       "      <td>0.961400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>4.043661e+06</td>\n",
       "      <td>27369</td>\n",
       "      <td>196152</td>\n",
       "      <td>0.38</td>\n",
       "      <td>14725</td>\n",
       "      <td>0.998574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>82</td>\n",
       "      <td>4596</td>\n",
       "      <td>701</td>\n",
       "      <td>582</td>\n",
       "      <td>1355</td>\n",
       "      <td>1610157</td>\n",
       "      <td>45</td>\n",
       "      <td>24.232902</td>\n",
       "      <td>48.265692</td>\n",
       "      <td>334.170111</td>\n",
       "      <td>...</td>\n",
       "      <td>1655.00</td>\n",
       "      <td>0.446833</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>8.895024e+05</td>\n",
       "      <td>5276</td>\n",
       "      <td>982097</td>\n",
       "      <td>0.42</td>\n",
       "      <td>433</td>\n",
       "      <td>0.990762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>83</td>\n",
       "      <td>1258</td>\n",
       "      <td>591</td>\n",
       "      <td>837</td>\n",
       "      <td>288</td>\n",
       "      <td>74641</td>\n",
       "      <td>40</td>\n",
       "      <td>100.546561</td>\n",
       "      <td>280.274603</td>\n",
       "      <td>789.412546</td>\n",
       "      <td>...</td>\n",
       "      <td>4956.00</td>\n",
       "      <td>0.890280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>6.134236e+05</td>\n",
       "      <td>5217</td>\n",
       "      <td>126321</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3693</td>\n",
       "      <td>0.997292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>84</td>\n",
       "      <td>3907</td>\n",
       "      <td>746</td>\n",
       "      <td>923</td>\n",
       "      <td>181</td>\n",
       "      <td>845670</td>\n",
       "      <td>46</td>\n",
       "      <td>157.464952</td>\n",
       "      <td>272.931064</td>\n",
       "      <td>1504.512407</td>\n",
       "      <td>...</td>\n",
       "      <td>8632.00</td>\n",
       "      <td>0.712241</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025154</td>\n",
       "      <td>5.959266e+05</td>\n",
       "      <td>4431</td>\n",
       "      <td>101025</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.997071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>85</td>\n",
       "      <td>1879</td>\n",
       "      <td>225</td>\n",
       "      <td>1068</td>\n",
       "      <td>597</td>\n",
       "      <td>20979</td>\n",
       "      <td>36</td>\n",
       "      <td>2535.611898</td>\n",
       "      <td>4096.645420</td>\n",
       "      <td>1321.220205</td>\n",
       "      <td>...</td>\n",
       "      <td>147279.00</td>\n",
       "      <td>1.024824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056124</td>\n",
       "      <td>1.216205e+06</td>\n",
       "      <td>6842</td>\n",
       "      <td>1411436</td>\n",
       "      <td>0.64</td>\n",
       "      <td>37949</td>\n",
       "      <td>0.996390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>86</td>\n",
       "      <td>985</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>110</td>\n",
       "      <td>196397</td>\n",
       "      <td>42</td>\n",
       "      <td>47.286290</td>\n",
       "      <td>2002.259073</td>\n",
       "      <td>3444.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>1296.00</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.572880e+05</td>\n",
       "      <td>1001</td>\n",
       "      <td>87361</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>87</td>\n",
       "      <td>3379</td>\n",
       "      <td>947</td>\n",
       "      <td>1231</td>\n",
       "      <td>433</td>\n",
       "      <td>484783</td>\n",
       "      <td>44</td>\n",
       "      <td>131.719031</td>\n",
       "      <td>122.490196</td>\n",
       "      <td>888.756202</td>\n",
       "      <td>...</td>\n",
       "      <td>10127.00</td>\n",
       "      <td>0.547518</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029120</td>\n",
       "      <td>1.880835e+05</td>\n",
       "      <td>982</td>\n",
       "      <td>186349</td>\n",
       "      <td>0.15</td>\n",
       "      <td>858</td>\n",
       "      <td>0.997669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>88</td>\n",
       "      <td>3280</td>\n",
       "      <td>382</td>\n",
       "      <td>1388</td>\n",
       "      <td>1069</td>\n",
       "      <td>1009730</td>\n",
       "      <td>47</td>\n",
       "      <td>389.433306</td>\n",
       "      <td>975.559414</td>\n",
       "      <td>1165.385985</td>\n",
       "      <td>...</td>\n",
       "      <td>74816.00</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060998</td>\n",
       "      <td>5.751977e+06</td>\n",
       "      <td>41581</td>\n",
       "      <td>473177</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33960</td>\n",
       "      <td>0.997585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>89</td>\n",
       "      <td>330</td>\n",
       "      <td>61</td>\n",
       "      <td>172</td>\n",
       "      <td>213</td>\n",
       "      <td>1356289</td>\n",
       "      <td>48</td>\n",
       "      <td>160.015113</td>\n",
       "      <td>177.367758</td>\n",
       "      <td>3195.895652</td>\n",
       "      <td>...</td>\n",
       "      <td>1136.00</td>\n",
       "      <td>0.638847</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>4.856608e+05</td>\n",
       "      <td>4974</td>\n",
       "      <td>624823</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.995480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>90</td>\n",
       "      <td>2668</td>\n",
       "      <td>1167</td>\n",
       "      <td>4443</td>\n",
       "      <td>393</td>\n",
       "      <td>6301549</td>\n",
       "      <td>46</td>\n",
       "      <td>150.146929</td>\n",
       "      <td>112.570873</td>\n",
       "      <td>231.668175</td>\n",
       "      <td>...</td>\n",
       "      <td>5616.00</td>\n",
       "      <td>1.015532</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058965</td>\n",
       "      <td>7.392115e+05</td>\n",
       "      <td>5111</td>\n",
       "      <td>1035698</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.992658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>91</td>\n",
       "      <td>3688</td>\n",
       "      <td>622</td>\n",
       "      <td>1530</td>\n",
       "      <td>85</td>\n",
       "      <td>701884</td>\n",
       "      <td>48</td>\n",
       "      <td>183.695771</td>\n",
       "      <td>164.345600</td>\n",
       "      <td>2273.846602</td>\n",
       "      <td>...</td>\n",
       "      <td>4558.00</td>\n",
       "      <td>0.336198</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>1.977980e+05</td>\n",
       "      <td>834</td>\n",
       "      <td>116269</td>\n",
       "      <td>0.41</td>\n",
       "      <td>684</td>\n",
       "      <td>0.994152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>92</td>\n",
       "      <td>897</td>\n",
       "      <td>355</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>669259</td>\n",
       "      <td>43</td>\n",
       "      <td>34.377463</td>\n",
       "      <td>13.874704</td>\n",
       "      <td>2688.400415</td>\n",
       "      <td>...</td>\n",
       "      <td>42400.00</td>\n",
       "      <td>1.135297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>3.410397e+06</td>\n",
       "      <td>34457</td>\n",
       "      <td>5806704</td>\n",
       "      <td>0.38</td>\n",
       "      <td>36429</td>\n",
       "      <td>0.995224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>93</td>\n",
       "      <td>1552</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>443</td>\n",
       "      <td>2190196</td>\n",
       "      <td>48</td>\n",
       "      <td>267.388191</td>\n",
       "      <td>721.353015</td>\n",
       "      <td>2579.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>1258.00</td>\n",
       "      <td>0.173437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.457550e+04</td>\n",
       "      <td>255</td>\n",
       "      <td>143220</td>\n",
       "      <td>0.50</td>\n",
       "      <td>182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>94</td>\n",
       "      <td>1079</td>\n",
       "      <td>122</td>\n",
       "      <td>1010</td>\n",
       "      <td>64</td>\n",
       "      <td>65828</td>\n",
       "      <td>9</td>\n",
       "      <td>24.632890</td>\n",
       "      <td>16.155316</td>\n",
       "      <td>34715.331850</td>\n",
       "      <td>...</td>\n",
       "      <td>19899.00</td>\n",
       "      <td>0.954421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050369</td>\n",
       "      <td>3.121026e+06</td>\n",
       "      <td>18701</td>\n",
       "      <td>498701</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4647</td>\n",
       "      <td>0.991607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>95</td>\n",
       "      <td>1496</td>\n",
       "      <td>516</td>\n",
       "      <td>34</td>\n",
       "      <td>384</td>\n",
       "      <td>909903</td>\n",
       "      <td>47</td>\n",
       "      <td>82.419960</td>\n",
       "      <td>51.661561</td>\n",
       "      <td>51.778598</td>\n",
       "      <td>...</td>\n",
       "      <td>4267.00</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052016</td>\n",
       "      <td>8.402770e+05</td>\n",
       "      <td>4190</td>\n",
       "      <td>140595</td>\n",
       "      <td>0.54</td>\n",
       "      <td>321</td>\n",
       "      <td>0.993769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>96</td>\n",
       "      <td>1851</td>\n",
       "      <td>567</td>\n",
       "      <td>196</td>\n",
       "      <td>266</td>\n",
       "      <td>676226</td>\n",
       "      <td>47</td>\n",
       "      <td>68.616288</td>\n",
       "      <td>60.179903</td>\n",
       "      <td>6426.206107</td>\n",
       "      <td>...</td>\n",
       "      <td>2823.00</td>\n",
       "      <td>0.583804</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>1.482059e+06</td>\n",
       "      <td>7496</td>\n",
       "      <td>153900</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3982</td>\n",
       "      <td>0.993973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>97</td>\n",
       "      <td>3395</td>\n",
       "      <td>1433</td>\n",
       "      <td>768</td>\n",
       "      <td>140</td>\n",
       "      <td>3789071</td>\n",
       "      <td>48</td>\n",
       "      <td>134.091963</td>\n",
       "      <td>172.692120</td>\n",
       "      <td>2156.632353</td>\n",
       "      <td>...</td>\n",
       "      <td>74827.00</td>\n",
       "      <td>0.967205</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062972</td>\n",
       "      <td>5.329175e+06</td>\n",
       "      <td>26839</td>\n",
       "      <td>279025</td>\n",
       "      <td>1.13</td>\n",
       "      <td>13694</td>\n",
       "      <td>0.995765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>99</td>\n",
       "      <td>1643</td>\n",
       "      <td>396</td>\n",
       "      <td>534</td>\n",
       "      <td>399</td>\n",
       "      <td>1472144</td>\n",
       "      <td>44</td>\n",
       "      <td>734.587416</td>\n",
       "      <td>10566.130160</td>\n",
       "      <td>3535.206630</td>\n",
       "      <td>...</td>\n",
       "      <td>2363.00</td>\n",
       "      <td>0.661086</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>6.426600e+04</td>\n",
       "      <td>297</td>\n",
       "      <td>9125</td>\n",
       "      <td>0.35</td>\n",
       "      <td>431</td>\n",
       "      <td>0.979118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>100</td>\n",
       "      <td>1823</td>\n",
       "      <td>80</td>\n",
       "      <td>657</td>\n",
       "      <td>277</td>\n",
       "      <td>4730191</td>\n",
       "      <td>41</td>\n",
       "      <td>1631.750131</td>\n",
       "      <td>896.561780</td>\n",
       "      <td>4291.062927</td>\n",
       "      <td>...</td>\n",
       "      <td>28207.00</td>\n",
       "      <td>0.841196</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061221</td>\n",
       "      <td>1.265697e+06</td>\n",
       "      <td>10516</td>\n",
       "      <td>258521</td>\n",
       "      <td>0.31</td>\n",
       "      <td>11133</td>\n",
       "      <td>0.997754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>102</td>\n",
       "      <td>1009</td>\n",
       "      <td>89</td>\n",
       "      <td>479</td>\n",
       "      <td>202</td>\n",
       "      <td>6738735</td>\n",
       "      <td>46</td>\n",
       "      <td>465.298819</td>\n",
       "      <td>539.878292</td>\n",
       "      <td>1214.898936</td>\n",
       "      <td>...</td>\n",
       "      <td>5206.00</td>\n",
       "      <td>0.780489</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060528</td>\n",
       "      <td>7.378105e+05</td>\n",
       "      <td>4164</td>\n",
       "      <td>231842</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4020</td>\n",
       "      <td>0.995025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>103</td>\n",
       "      <td>235</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>802</td>\n",
       "      <td>2901026</td>\n",
       "      <td>47</td>\n",
       "      <td>6.557229</td>\n",
       "      <td>4.837349</td>\n",
       "      <td>99.822917</td>\n",
       "      <td>...</td>\n",
       "      <td>163028.00</td>\n",
       "      <td>0.918368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>4.246669e+07</td>\n",
       "      <td>56654</td>\n",
       "      <td>943518</td>\n",
       "      <td>0.41</td>\n",
       "      <td>38345</td>\n",
       "      <td>0.998696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>104</td>\n",
       "      <td>1346</td>\n",
       "      <td>704</td>\n",
       "      <td>1036</td>\n",
       "      <td>429</td>\n",
       "      <td>660991</td>\n",
       "      <td>44</td>\n",
       "      <td>130.548109</td>\n",
       "      <td>197.515558</td>\n",
       "      <td>478.990006</td>\n",
       "      <td>...</td>\n",
       "      <td>7293.00</td>\n",
       "      <td>0.529110</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048054</td>\n",
       "      <td>1.156734e+06</td>\n",
       "      <td>7009</td>\n",
       "      <td>312013</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1079</td>\n",
       "      <td>0.965709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>105</td>\n",
       "      <td>1446</td>\n",
       "      <td>108</td>\n",
       "      <td>1118</td>\n",
       "      <td>784</td>\n",
       "      <td>4966250</td>\n",
       "      <td>46</td>\n",
       "      <td>230.932735</td>\n",
       "      <td>229.213325</td>\n",
       "      <td>906.077869</td>\n",
       "      <td>...</td>\n",
       "      <td>15113.00</td>\n",
       "      <td>0.741922</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>1.141174e+06</td>\n",
       "      <td>10611</td>\n",
       "      <td>1261130</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9578</td>\n",
       "      <td>0.998016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>108</td>\n",
       "      <td>3057</td>\n",
       "      <td>255</td>\n",
       "      <td>546</td>\n",
       "      <td>249</td>\n",
       "      <td>1615138</td>\n",
       "      <td>44</td>\n",
       "      <td>640.074063</td>\n",
       "      <td>971.754074</td>\n",
       "      <td>1551.035433</td>\n",
       "      <td>...</td>\n",
       "      <td>33953.00</td>\n",
       "      <td>0.604264</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044246</td>\n",
       "      <td>1.317523e+07</td>\n",
       "      <td>133717</td>\n",
       "      <td>335153</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4341</td>\n",
       "      <td>0.987100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ï»¿userid  original_weibo  repost_weibo  like_weibo  follow     fans  \\\n",
       "0         1            2834           679        1343     316  3775598   \n",
       "1         3            4082          1164        1062     393  4348114   \n",
       "2         5            4050          2285         538     751   778999   \n",
       "3         6            1351           766         262     334  1241240   \n",
       "4         7            3772          1145        1917     701  2498820   \n",
       "5         8            2457          1126         696     442  3270155   \n",
       "6         9            3605          1438         120     759  2295703   \n",
       "7        10            2199           211         272     181  5733434   \n",
       "8        12            1369           515        2318     183  2879664   \n",
       "9        13            2025           477         928     567   502984   \n",
       "10       14            2850           245        1952     147   387598   \n",
       "11       15            3005           586         818     768  1358200   \n",
       "12       16            2120           408         926     695  4095687   \n",
       "13       17            3238          1007         993     615  2775128   \n",
       "14       18            7514          3470        2735     458    84325   \n",
       "15       19            2006           530          12     509   346674   \n",
       "16       20            2572          2398        2434     281  2010240   \n",
       "17       21            1970           937         617     182   137185   \n",
       "18       22            1316           142         987      10    67843   \n",
       "19       23             589           244         100     277    11725   \n",
       "20       24             794           155         636     219   513576   \n",
       "21       25            2108           553         746     464    53265   \n",
       "22       27             766            44           9     222  1393041   \n",
       "23       28            1673           554         183     266   613758   \n",
       "24       29            1825           171         246     112   731378   \n",
       "25       32             397           151         236     965   160499   \n",
       "26       33             301           368         237     729   476635   \n",
       "27       34             889           169         148      88  1701109   \n",
       "28       35            1255           591        4279     144   339578   \n",
       "29       37             606            28         408     215   169289   \n",
       "..      ...             ...           ...         ...     ...      ...   \n",
       "54       73             785           126          70     582   966023   \n",
       "55       75            1690           123        4024     967   978416   \n",
       "56       76            4030          1876         480     262  3660599   \n",
       "57       77            3088          1539         380     370  1169908   \n",
       "58       79            5578          4167        1333     275  1719900   \n",
       "59       80            1105            52         625     218   250745   \n",
       "60       81            1627           457         472     897  1181707   \n",
       "61       82            4596           701         582    1355  1610157   \n",
       "62       83            1258           591         837     288    74641   \n",
       "63       84            3907           746         923     181   845670   \n",
       "64       85            1879           225        1068     597    20979   \n",
       "65       86             985             7          29     110   196397   \n",
       "66       87            3379           947        1231     433   484783   \n",
       "67       88            3280           382        1388    1069  1009730   \n",
       "68       89             330            61         172     213  1356289   \n",
       "69       90            2668          1167        4443     393  6301549   \n",
       "70       91            3688           622        1530      85   701884   \n",
       "71       92             897           355         143       2   669259   \n",
       "72       93            1552            37          27     443  2190196   \n",
       "73       94            1079           122        1010      64    65828   \n",
       "74       95            1496           516          34     384   909903   \n",
       "75       96            1851           567         196     266   676226   \n",
       "76       97            3395          1433         768     140  3789071   \n",
       "77       99            1643           396         534     399  1472144   \n",
       "78      100            1823            80         657     277  4730191   \n",
       "79      102            1009            89         479     202  6738735   \n",
       "80      103             235            69          32     802  2901026   \n",
       "81      104            1346           704        1036     429   660991   \n",
       "82      105            1446           108        1118     784  4966250   \n",
       "83      108            3057           255         546     249  1615138   \n",
       "\n",
       "    vip_level   av_comment       av_like  av_re_comment    ...     \\\n",
       "0          45   703.142214   1150.808223    3551.350806    ...      \n",
       "1          48    83.097712    396.988982    1060.471216    ...      \n",
       "2          48     9.862074    248.611844     114.159870    ...      \n",
       "3          43   257.978234   2177.106605     370.986234    ...      \n",
       "4          48   733.688423    996.604790    1113.772453    ...      \n",
       "5          43   221.039438    460.953958     234.175818    ...      \n",
       "6          48   117.579598    262.434421     334.918314    ...      \n",
       "7          48  3142.379453   7556.855960    2279.682713    ...      \n",
       "8          48  1831.429379    865.555727    2986.545552    ...      \n",
       "9          46   188.166470    988.913420    1336.992481    ...      \n",
       "10         41   866.134926    654.452550    1631.643773    ...      \n",
       "11         44   360.139734    171.499447     835.014001    ...      \n",
       "12         48  1999.103931   4214.670689    2023.294770    ...      \n",
       "13         45   410.918386    329.528171     571.432974    ...      \n",
       "14         41   587.342612    677.125547    1144.501976    ...      \n",
       "15         47   281.138243    538.023997     537.522484    ...      \n",
       "16         45    70.314722   1402.445561     397.879566    ...      \n",
       "17         48   303.773913    239.317848     214.834958    ...      \n",
       "18         43   213.211853    105.755450    1112.059783    ...      \n",
       "19         40    19.427746    271.838117     355.418398    ...      \n",
       "20         46  1150.535975    879.478624    1962.353393    ...      \n",
       "21         48   167.710166    492.197412     263.559969    ...      \n",
       "22         41   214.612469    106.442543    1602.450980    ...      \n",
       "23         42   131.358681     96.560000    1766.012676    ...      \n",
       "24         45  2820.496210   2486.370055    1639.251244    ...      \n",
       "25         41    15.322183      4.103873    9491.859838    ...      \n",
       "26         48   114.324693    179.536153     453.633613    ...      \n",
       "27         41     7.535014     11.290383    6316.531353    ...      \n",
       "28         43   135.165486    453.283907    6124.331541    ...      \n",
       "29         47   381.339090    356.163265    5331.381944    ...      \n",
       "..        ...          ...           ...            ...    ...      \n",
       "54         48  2805.534276   3327.886834    8834.175532    ...      \n",
       "55         48  3832.375138  16229.174370    3039.881455    ...      \n",
       "56         48    31.831060     67.575349    2248.821786    ...      \n",
       "57         46    54.197901     40.970884    1103.936782    ...      \n",
       "58         47   110.445045     48.684866     746.712513    ...      \n",
       "59         38   293.062772   1943.590517     559.653274    ...      \n",
       "60         48   741.445026   1428.446930    2259.100220    ...      \n",
       "61         45    24.232902     48.265692     334.170111    ...      \n",
       "62         40   100.546561    280.274603     789.412546    ...      \n",
       "63         46   157.464952    272.931064    1504.512407    ...      \n",
       "64         36  2535.611898   4096.645420    1321.220205    ...      \n",
       "65         42    47.286290   2002.259073    3444.583333    ...      \n",
       "66         44   131.719031    122.490196     888.756202    ...      \n",
       "67         47   389.433306    975.559414    1165.385985    ...      \n",
       "68         48   160.015113    177.367758    3195.895652    ...      \n",
       "69         46   150.146929    112.570873     231.668175    ...      \n",
       "70         48   183.695771    164.345600    2273.846602    ...      \n",
       "71         43    34.377463     13.874704    2688.400415    ...      \n",
       "72         48   267.388191    721.353015    2579.966667    ...      \n",
       "73          9    24.632890     16.155316   34715.331850    ...      \n",
       "74         47    82.419960     51.661561      51.778598    ...      \n",
       "75         47    68.616288     60.179903    6426.206107    ...      \n",
       "76         48   134.091963    172.692120    2156.632353    ...      \n",
       "77         44   734.587416  10566.130160    3535.206630    ...      \n",
       "78         41  1631.750131    896.561780    4291.062927    ...      \n",
       "79         46   465.298819    539.878292    1214.898936    ...      \n",
       "80         47     6.557229      4.837349      99.822917    ...      \n",
       "81         44   130.548109    197.515558     478.990006    ...      \n",
       "82         46   230.932735    229.213325     906.077869    ...      \n",
       "83         44   640.074063    971.754074    1551.035433    ...      \n",
       "\n",
       "    product_comm  product_comm_sent  update  neg_com_perc  \\\n",
       "0       21346.00           1.013699       0      0.041571   \n",
       "1       81867.00           0.875705       1      0.032215   \n",
       "2      124781.00           0.906117       0      0.055203   \n",
       "3      309598.00           1.018530       1      0.051287   \n",
       "4       25911.00           0.846137       1      0.049384   \n",
       "5       33467.00           0.918922       1      0.044797   \n",
       "6       45051.00           0.922688       0      0.058252   \n",
       "7      334594.00           1.206648       1      0.037682   \n",
       "8      139566.00           0.989010       1      0.046109   \n",
       "9       97986.00           0.926261       0      0.062744   \n",
       "10     304884.00           1.009798       1      0.053261   \n",
       "11      27745.00           0.761522       1      0.041215   \n",
       "12     166073.00           1.141223       1      0.043542   \n",
       "13     497827.00           1.001447       1      0.064905   \n",
       "14     810179.00           1.134226       0      0.054761   \n",
       "15      40058.00           0.744758       1      0.056316   \n",
       "16     639770.00           1.051818       1      0.064702   \n",
       "17      12026.00           0.917028       0      0.043321   \n",
       "18       7167.00           0.712781       1      0.029142   \n",
       "19      15792.00           0.656400       0      0.071461   \n",
       "20       4209.00           0.393804       0      0.033882   \n",
       "21     139167.00           0.808987       0      0.038585   \n",
       "22      46192.00           0.891414       1      0.043740   \n",
       "23       2104.00           0.715268       1      0.045230   \n",
       "24     219703.00           0.894031       1      0.079447   \n",
       "25   63145160.02           1.442884       1      0.035580   \n",
       "26      18075.00           0.918081       0      0.055143   \n",
       "27       2291.00           0.868668       0      0.051687   \n",
       "28     147129.00           0.885049       1      0.046152   \n",
       "29       9742.00           0.539108       1      0.061203   \n",
       "..           ...                ...     ...           ...   \n",
       "54       1295.00           0.917465       0      0.030986   \n",
       "55      17897.00           0.831589       1      0.073037   \n",
       "56      12861.00           0.521505       0      0.022036   \n",
       "57      41346.00           0.958541       0      0.064628   \n",
       "58      26007.00           0.853030       0      0.112284   \n",
       "59       4312.00           0.127260       0      0.001360   \n",
       "60      59902.00           0.961400       1      0.055500   \n",
       "61       1655.00           0.446833       1      0.031674   \n",
       "62       4956.00           0.890280       0      0.041186   \n",
       "63       8632.00           0.712241       1      0.025154   \n",
       "64     147279.00           1.024824       1      0.056124   \n",
       "65       1296.00           0.470000       0      0.050000   \n",
       "66      10127.00           0.547518       1      0.029120   \n",
       "67      74816.00           0.999899       1      0.060998   \n",
       "68       1136.00           0.638847       0      0.017544   \n",
       "69       5616.00           1.015532       1      0.058965   \n",
       "70       4558.00           0.336198       1      0.014876   \n",
       "71      42400.00           1.135297       0      0.061482   \n",
       "72       1258.00           0.173437       0      0.007812   \n",
       "73      19899.00           0.954421       0      0.050369   \n",
       "74       4267.00           0.711864       0      0.052016   \n",
       "75       2823.00           0.583804       0      0.035583   \n",
       "76      74827.00           0.967205       1      0.062972   \n",
       "77       2363.00           0.661086       1      0.016591   \n",
       "78      28207.00           0.841196       0      0.061221   \n",
       "79       5206.00           0.780489       0      0.060528   \n",
       "80     163028.00           0.918368       0      0.046892   \n",
       "81       7293.00           0.529110       1      0.048054   \n",
       "82      15113.00           0.741922       1      0.027239   \n",
       "83      33953.00           0.604264       0      0.044246   \n",
       "\n",
       "    turnover_month_appr  sales_month    credit  refund_rate  comment_num  \\\n",
       "0          1.914995e+06         2778    893935         0.80         3333   \n",
       "1          1.797427e+07        54238    826952         0.55        22529   \n",
       "2          1.558100e+07        92442   2288782         0.32        40605   \n",
       "3          1.654340e+07       121452   4169378         0.41        58071   \n",
       "4          3.546000e+06        20365    945335         0.39        12194   \n",
       "5          1.395485e+07        96870   1635805         0.41        17517   \n",
       "6          6.610076e+06        26396   1278608         0.82        17076   \n",
       "7          3.598865e+07       237754   2317829         0.38       154830   \n",
       "8          2.506724e+06        15913   2435007         0.91        74413   \n",
       "9          1.879275e+07        64978   1195232         0.40        36685   \n",
       "10         3.006963e+07       168148   1896732         0.53       124381   \n",
       "11         1.357244e+06         6330    192088         1.19         4309   \n",
       "12         3.654729e+07       282725   4380269         0.45       198728   \n",
       "13         2.592427e+07       156271   3729398         0.50        55984   \n",
       "14         5.748386e+07       303797   3405924         0.44       134206   \n",
       "15         1.316547e+07        78516   1989581         0.73         9419   \n",
       "16         1.448977e+08       307505   2607424         0.56        29162   \n",
       "17         1.755535e+07        30511    451809         0.49         4147   \n",
       "18         6.875260e+05         4700    139380         0.26          884   \n",
       "19         2.262100e+06         6838    314468         0.52         1277   \n",
       "20         2.083316e+06        10522    236781         0.76         7049   \n",
       "21         1.534274e+07        47315    820393         0.48        36022   \n",
       "22         3.049598e+06        18306    551169         0.27         9784   \n",
       "23         2.831217e+05         1180     87509         0.37         1275   \n",
       "24         1.207810e+07        93160   1040827         0.28        43586   \n",
       "25         3.157258e+07       326284  11289676         0.38       233716   \n",
       "26         5.319155e+05         1031    231133         0.17          597   \n",
       "27         1.100708e+05          502    118972         0.28          262   \n",
       "28         6.757381e+06        38450    764369         0.28        46290   \n",
       "29         1.120556e+05          614    368959         0.28         2452   \n",
       "..                  ...          ...       ...          ...          ...   \n",
       "54         4.833896e+05         1725      6441         0.38          258   \n",
       "55         9.024420e+05         9153    354968         0.23        14313   \n",
       "56         2.048860e+06         2736    231472         0.29         2415   \n",
       "57         1.301925e+06          981    200038         0.40         1296   \n",
       "58         5.266697e+06        20087    322299         0.19         3071   \n",
       "59         2.820965e+05         3125    118292         1.05           79   \n",
       "60         4.043661e+06        27369    196152         0.38        14725   \n",
       "61         8.895024e+05         5276    982097         0.42          433   \n",
       "62         6.134236e+05         5217    126321         0.33         3693   \n",
       "63         5.959266e+05         4431    101025         0.35         1707   \n",
       "64         1.216205e+06         6842   1411436         0.64        37949   \n",
       "65         2.572880e+05         1001     87361         0.49            9   \n",
       "66         1.880835e+05          982    186349         0.15          858   \n",
       "67         5.751977e+06        41581    473177         0.43        33960   \n",
       "68         4.856608e+05         4974    624823         0.31         1991   \n",
       "69         7.392115e+05         5111   1035698         0.36         1362   \n",
       "70         1.977980e+05          834    116269         0.41          684   \n",
       "71         3.410397e+06        34457   5806704         0.38        36429   \n",
       "72         2.457550e+04          255    143220         0.50          182   \n",
       "73         3.121026e+06        18701    498701         0.38         4647   \n",
       "74         8.402770e+05         4190    140595         0.54          321   \n",
       "75         1.482059e+06         7496    153900         0.78         3982   \n",
       "76         5.329175e+06        26839    279025         1.13        13694   \n",
       "77         6.426600e+04          297      9125         0.35          431   \n",
       "78         1.265697e+06        10516    258521         0.31        11133   \n",
       "79         7.378105e+05         4164    231842         0.33         4020   \n",
       "80         4.246669e+07        56654    943518         0.41        38345   \n",
       "81         1.156734e+06         7009    312013         0.45         1079   \n",
       "82         1.141174e+06        10611   1261130         0.30         9578   \n",
       "83         1.317523e+07       133717    335153         0.37         4341   \n",
       "\n",
       "    com_perc  \n",
       "0   0.986199  \n",
       "1   0.998003  \n",
       "2   0.996725  \n",
       "3   0.998640  \n",
       "4   0.989339  \n",
       "5   0.992750  \n",
       "6   0.997599  \n",
       "7   0.997462  \n",
       "8   0.996210  \n",
       "9   0.998446  \n",
       "10  0.997339  \n",
       "11  0.996751  \n",
       "12  0.996820  \n",
       "13  0.994338  \n",
       "14  0.999076  \n",
       "15  0.993205  \n",
       "16  0.990810  \n",
       "17  0.996383  \n",
       "18  0.994344  \n",
       "19  0.995301  \n",
       "20  0.996170  \n",
       "21  0.998862  \n",
       "22  0.999285  \n",
       "23  1.000000  \n",
       "24  0.998325  \n",
       "25  0.990232  \n",
       "26  1.000000  \n",
       "27  0.996183  \n",
       "28  0.998661  \n",
       "29  0.998777  \n",
       "..       ...  \n",
       "54  0.996124  \n",
       "55  0.994481  \n",
       "56  0.997930  \n",
       "57  1.000000  \n",
       "58  0.997069  \n",
       "59  0.924051  \n",
       "60  0.998574  \n",
       "61  0.990762  \n",
       "62  0.997292  \n",
       "63  0.997071  \n",
       "64  0.996390  \n",
       "65  1.000000  \n",
       "66  0.997669  \n",
       "67  0.997585  \n",
       "68  0.995480  \n",
       "69  0.992658  \n",
       "70  0.994152  \n",
       "71  0.995224  \n",
       "72  1.000000  \n",
       "73  0.991607  \n",
       "74  0.993769  \n",
       "75  0.993973  \n",
       "76  0.995765  \n",
       "77  0.979118  \n",
       "78  0.997754  \n",
       "79  0.995025  \n",
       "80  0.998696  \n",
       "81  0.965709  \n",
       "82  0.998016  \n",
       "83  0.987100  \n",
       "\n",
       "[84 rows x 26 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# from astropy.units import Ybarn\n",
    "from pylab import *\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "% matplotlib inline\n",
    "\n",
    "gongzuobu = pd.read_csv('newfeatures.csv')\n",
    "gongzuobu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    if x<43000 -75000/2:\n",
    "        return 1\n",
    "    if (x>= 43000 - 75000/2) and (x < 43000 + 75000/2):\n",
    "        return 2\n",
    "    if x>=43000 + 75000/2:\n",
    "        return 3\n",
    "\n",
    "\n",
    "\n",
    "gongzuobu['label'] = gongzuobu['sales_month'].map(lambda x : get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = gongzuobu[[\n",
    "               'product_comm_sent','neg_com_perc','ave_price','price_var','update',\n",
    "                'original_weibo','repost_weibo','like_weibo','fans','av_comment','av_like','av_repost','follow'\n",
    "               ]]\n",
    "y = gongzuobu['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y=label_binarize(list(y), classes=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633333333333\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size=0.4, random_state=0)\n",
    "rf0 = RandomForestClassifier(oob_score=True, random_state=10)  \n",
    "rf0.fit(X_train,y_train)  \n",
    "print (rf0.oob_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 0 (0.329110)\n",
      "2. feature 3 (0.238425)\n",
      "3. feature 2 (0.235091)\n",
      "4. feature 1 (0.127523)\n",
      "5. feature 4 (0.069851)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Y324XmOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6de1400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rf0.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf0.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "width=0.5\n",
    "plt.bar(range(X.shape[1]), importances[indices],width=width,color=\"r\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.savefig('C:/Users/Yee Shen/Desktop/source/figure/taobao&weibo.png',dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf0.predict(X_test)  # è¿éæç®åçæåµå®éªæåã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.40000, std: 0.22775, params: {'n_estimators': 10},\n",
       "  mean: 0.36000, std: 0.19947, params: {'n_estimators': 20},\n",
       "  mean: 0.38000, std: 0.20514, params: {'n_estimators': 30},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 40},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 50},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 60},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 70},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 80},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 90},\n",
       "  mean: 0.36000, std: 0.23168, params: {'n_estimators': 100}],\n",
       " {'n_estimators': 10},\n",
       " 0.40000000000000002)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [{'n_estimators': list(range(10, 101, 10))}]\n",
    "gsearch1 = GridSearchCV( estimator = RandomForestClassifier(oob_score=True, random_state=10),\n",
    "                         param_grid = param, scoring = 'accuracy',cv = 8)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and use averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |      Ignored if ``max_leaf_nodes`` is not None.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_samples_split : integer, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_samples_leaf : integer, optional (default=1)\n",
      " |      The minimum number of samples in newly created leaves.  A split is\n",
      " |      discarded if after the split, one of the leaves would contain less then\n",
      " |      ``min_samples_leaf`` samples.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the input samples required to be at a\n",
      " |      leaf node.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |      If not None then ``max_depth`` will be ignored.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization error.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"auto\", \"subsample\" or None, optional\n",
      " |  \n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      The \"auto\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data.\n",
      " |  \n",
      " |      The \"subsample\" mode is the same as \"auto\" except that weights are\n",
      " |      computed based on the bootstrap sample for every tree grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the majority\n",
      " |      prediction of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      " |  \n",
      " |  transform(self, X, threshold=None)\n",
      " |      Reduce X to its most important features.\n",
      " |      \n",
      " |      Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      " |      important features.  For models with a ``coef_`` for each class, the\n",
      " |      absolute sum over the classes is used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      threshold : string, float or None, optional (default=None)\n",
      " |          The threshold value to use for feature selection. Features whose\n",
      " |          importance is greater or equal are kept while the others are\n",
      " |          discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      " |          the median (resp. the mean) of the feature importances. A scaling\n",
      " |          factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      " |          available, the object attribute ``threshold`` is used. Otherwise,\n",
      " |          \"mean\" is used by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.36000, std: 0.30725, params: {'min_samples_split': 2},\n",
       "  mean: 0.40000, std: 0.25298, params: {'min_samples_split': 3},\n",
       "  mean: 0.48000, std: 0.22271, params: {'min_samples_split': 4},\n",
       "  mean: 0.38000, std: 0.22716, params: {'min_samples_split': 5},\n",
       "  mean: 0.40000, std: 0.20000, params: {'min_samples_split': 6},\n",
       "  mean: 0.42000, std: 0.27495, params: {'min_samples_split': 7},\n",
       "  mean: 0.38000, std: 0.14000, params: {'min_samples_split': 8},\n",
       "  mean: 0.32000, std: 0.24000, params: {'min_samples_split': 9},\n",
       "  mean: 0.40000, std: 0.25298, params: {'min_samples_split': 10}],\n",
       " {'min_samples_split': 4},\n",
       " 0.47999999999999998)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [{'min_samples_split':list(range(2, 11, 1))}]\n",
    "gsearch1 = GridSearchCV( RandomForestClassifier( ),param_grid = param, scoring = 'accuracy',cv=10)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.56000, std: 0.17436, params: {'min_samples_leaf': 10},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 20},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 30},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 40},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 50},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 60},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 70},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 80},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 90},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 100}],\n",
       " {'min_samples_leaf': 20},\n",
       " 0.59999999999999998)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [{'min_samples_leaf':list(range(10, 101, 10))}]\n",
    "gsearch1 = GridSearchCV( RandomForestClassifier( ),param_grid = param, scoring = 'accuracy',cv=10)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.54000, std: 0.15620, params: {'min_samples_leaf': 10},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 20},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 30},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 40},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 50},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 60},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 70},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 80},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 90},\n",
       "  mean: 0.60000, std: 0.20000, params: {'min_samples_leaf': 100}],\n",
       " {'min_samples_leaf': 20},\n",
       " 0.59999999999999998)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [{'min_samples_leaf':list(range(10, 101, 10))}]\n",
    "gsearch1 = GridSearchCV( RandomForestClassifier( ),param_grid = param, scoring = 'accuracy',cv=10)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
